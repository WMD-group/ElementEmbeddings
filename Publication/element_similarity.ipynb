{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element Similarity\n",
    "\n",
    "This notebook is used to reproduce the plots shown in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Tuple\n",
    "from elementembeddings.core import Embedding, data_directory\n",
    "from elementembeddings.plotter import dimension_plotter, heatmap_plotter\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's set up the Embedding classes and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "cbfvs = [\n",
    "    \"magpie_sc\",\n",
    "    \"magpie\",\n",
    "    \"mat2vec\",\n",
    "    \"matscholar\",\n",
    "    \"megnet16\",\n",
    "    \"oliynyk\",\n",
    "    \"mod_petti\",\n",
    "    \"random_200\",\n",
    "    \"skipatom\",\n",
    "]\n",
    "element_embedddings = {cbfv: Embedding.load_data(cbfv) for cbfv in cbfvs}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reproduce some of the information in table I from the paper by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the dimensionality of all of the CBFVs that we have loaded\n",
    "\n",
    "element_embedddings_dim = {cbfv: [element_embedddings[cbfv].dim] for cbfv in cbfvs}\n",
    "\n",
    "dim_df = pd.DataFrame.from_dict(\n",
    "    element_embedddings_dim, orient=\"index\", columns=[\"dimension\"]\n",
    ")\n",
    "print(dim_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.B Similarity measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the Embedding classes for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our four embeddings to compare\n",
    "cbfvs_to_keep = [\"magpie_sc\", \"mat2vec\", \"megnet16\", \"random_200\"]\n",
    "element_vectors = {cbfv: element_embedddings[cbfv] for cbfv in cbfvs_to_keep}\n",
    "\n",
    "# Keep the first 83 elements\n",
    "\n",
    "# Get the ordered symbols file\n",
    "symbols_path = os.path.join(data_directory, \"element_data\", \"ordered_periodic.txt\")\n",
    "with open(symbols_path) as f:\n",
    "    symbols = f.read().splitlines()\n",
    "\n",
    "# Get the first 83 elements\n",
    "symbols = symbols[:83]\n",
    "\n",
    "for cbfv in cbfvs_to_keep:\n",
    "    # Get the keys of the atomic embeddings object\n",
    "    elements = set(element_vectors[cbfv].element_list)\n",
    "    el_symbols_set = set(symbols)\n",
    "\n",
    "    # Get the element symbols we want to remove\n",
    "    els_to_remove = list(elements - el_symbols_set)\n",
    "\n",
    "    # Iteratively delete the elements with atomic number\n",
    "    # greater than 83 from our embeddings\n",
    "    for el in els_to_remove:\n",
    "        del element_vectors[cbfv].embeddings[el]\n",
    "\n",
    "    # Verify that we have 83 elements\n",
    "    print(len(element_vectors[cbfv].element_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances and similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [\"euclidean\", \"manhattan\", \"chebyshev\"]\n",
    "for distance in distances:\n",
    "    d = element_embedddings[\"magpie_sc\"].compute_distance_metric(\"Li\", \"K\", distance)\n",
    "    print(f\"Distance between Li and K using {distance} is {d:.2f}\")\n",
    "\n",
    "# Get the pearson correlation and cosine similarity between the embeddings for Li and K\n",
    "similarity_metrics = [\"pearson\", \"cosine_similarity\"]\n",
    "for similarity_metric in similarity_metrics:\n",
    "    magpie_d = element_embedddings[\"magpie_sc\"].compute_correlation_metric(\n",
    "        \"Li\", \"K\", similarity_metric\n",
    "    )\n",
    "\n",
    "    magpie_d_Li_Bi = element_embedddings[\"magpie_sc\"].compute_correlation_metric(\n",
    "        \"Li\", \"Bi\", similarity_metric\n",
    "    )\n",
    "\n",
    "    mvec_d = element_embedddings[\"mat2vec\"].compute_correlation_metric(\n",
    "        \"Li\", \"K\", similarity_metric\n",
    "    )\n",
    "    mvec_d_Li_Bi = element_embedddings[\"mat2vec\"].compute_correlation_metric(\n",
    "        \"Li\", \"Bi\", similarity_metric\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"The metric, {similarity_metric}, between Li and K is {magpie_d:.3f} for magpie and {mvec_d:.3f} for mat2vec\"\n",
    "    )\n",
    "    print(\n",
    "        f\"The metric, {similarity_metric}, between Li and Bi is {magpie_d_Li_Bi:.3f} for magpie and {mvec_d_Li_Bi:.3f} for mat2vec\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distances\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "d_E(\\textbf{A,B}) = \n",
    "\\sqrt{\n",
    "(A_1 - B_1)^2 \n",
    "+ \\cdots\n",
    "+ (A_n - B_n)^2 }\n",
    "\\end{equation}\n",
    "\n",
    "We can use the Euclidean distance to compare the similarity of two elements. The following code will plot the distribution of the Euclidean distances between all pairs of elements in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axes) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    heatmap_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        metric=\"euclidean\",\n",
    "        sortaxisby=\"atomic_number\",\n",
    "        show_axislabels=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1_euclidean.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan distances\n",
    "\n",
    "\\begin{equation}\n",
    "d_M(\\textbf{A,B}) = \n",
    "\\sum_{i=1}^n |A_i - B_i|\n",
    "\\end{equation}\n",
    "\n",
    "We can use the Manhattan distance to compare the similarity of two elements. The following code will plot the distribution of the Manhattan distances between all pairs of elements in the embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axes) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    heatmap_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        metric=\"manhattan\",\n",
    "        sortaxisby=\"atomic_number\",\n",
    "        show_axislabels=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2_manhattan.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "\\begin{equation}\n",
    "cos(\\theta) = \\frac{\\textbf{A} \\cdot \\textbf{B}} {||\\textbf{A}|| ||\\textbf{B}||}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axes) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "heatmap_params = {\"vmin\": -1, \"vmax\": 1}\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    heatmap_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        metric=\"cosine_similarity\",\n",
    "        sortaxisby=\"atomic_number\",\n",
    "        show_axislabels=False,\n",
    "        cmap=\"Blues_r\",\n",
    "        ax=ax,\n",
    "        **heatmap_params\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"3_cosine_similarity.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{A,B} = \\frac{cov(A,B)}{\\sigma_{A}\\sigma_{B}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axes) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "heatmap_params = {\"vmin\": -1, \"vmax\": 1}\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    heatmap_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        metric=\"pearson\",\n",
    "        sortaxisby=\"atomic_number\",\n",
    "        show_axislabels=False,\n",
    "        cmap=\"Blues_r\",\n",
    "        ax=ax,\n",
    "        **heatmap_params\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"4_pearson.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.C Dimensionality reduction\n",
    "To visualise the embeddings, we can use dimensionality reduction techniques such as PCA and t-SNE. The following code will plot the embeddings in 2D using PCA, t-SNE and UMAP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "The main concept behind PCA is to reduce the dimensionality of a dataset consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2,\n",
    "    2,\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    dimension_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        reducer=\"pca\",\n",
    "        n_components=2,\n",
    "        ax=ax,\n",
    "        adjusttext=True,\n",
    "    )\n",
    "    ax.legend().remove()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.54, 1.06), loc=\"upper center\", ncol=5)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"5_pca.pdf\", bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a non-linear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2,\n",
    "    2,\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    dimension_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        reducer=\"tsne\",\n",
    "        n_components=2,\n",
    "        ax=ax,\n",
    "        adjusttext=True,\n",
    "    )\n",
    "    ax.legend().remove()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.54, 1.06), loc=\"upper center\", ncol=5)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"6_tsne.pdf\", bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "UMAP is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data: the data is uniformly distributed on a Riemannian manifold, the Riemannian metric is locally constant, and the manifold is locally connected. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2,\n",
    "    2,\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    dimension_plotter(\n",
    "        embedding=element_vectors[cbfv],\n",
    "        reducer=\"umap\",\n",
    "        n_components=2,\n",
    "        ax=ax,\n",
    "        adjusttext=True,\n",
    "    )\n",
    "    ax.legend().remove()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.54, 1.06), loc=\"upper center\", ncol=5)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"7_umap.pdf\", bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribtion plot\n",
    "# Create dictionaries to store the correlation dataframes for the embeddings for each metric\n",
    "correlation_metrics = [\"pearson\", \"cosine_similarity\"]\n",
    "correlation_dfs = {}\n",
    "for cbfv in cbfvs_to_keep:\n",
    "    correlation_dfs[cbfv] = {\n",
    "        \"pearson\": element_vectors[cbfv].correlation_df(),\n",
    "        \"cosine_similarity\": element_vectors[cbfv].correlation_df(\n",
    "            metric=\"cosine_similarity\"\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    sns.histplot(correlation_dfs[cbfv][\"pearson\"], x=\"pearson\", ax=ax)\n",
    "    ax.set_title(cbfv)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_xlabel(\"Pearson correlation\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SI_pearson_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for ax, cbfv in zip(axes.flatten(), cbfvs_to_keep):\n",
    "    sns.histplot(\n",
    "        correlation_dfs[cbfv][\"cosine_similarity\"], x=\"cosine_similarity\", ax=ax\n",
    "    )\n",
    "    ax.set_title(cbfv)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_xlabel(\"Cosine similarity\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SI_cosine_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomic_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b27d7815fced387b88df5b0ff93cedd6822b989d46e35c5073559e46421f5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
